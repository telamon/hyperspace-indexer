const sbd = require('sbd').sentences
const { PATH_SEP, MDLINK_EXP } = require('./util')

module.exports = async function analyzeText ({ drive, readRemoteFile, log, writeEntry, index, setPreview, seq }, file) {
  if (file.match(/\/?node_modules\/./)) return // avoid analyzing things from NPM.
  const { stat, body } = await readRemoteFile(drive, file, 1024 * 1024)
  if (!body) return

  const text = body.toString('utf8')
  const driveKey = drive.key.hexSlice()
  const outgoingLinks = []

  //TODO: let /updates index copy 'beaker/subject' metadata.

  // Find via links in metadata
  if (file.match(/^comments\/./) && stat.metadata && stat.metadata['beaker/subject']) {
    const subject = stat.metadata['beaker/subject'].toString()
    if (subject.startsWith('hyper://')) outgoingLinks.push(subject)
    const key = `comments/${subject}/${driveKey}_${file.replace(/\//g, PATH_SEP)}`
    debugger
  } else return // tmp

  // Discover new drives via hyperlinking
  for (const link of text.matchAll(new RegExp(MDLINK_EXP, 'g'))) {
    if (link[2].match(/^\.?\.?\//)) link[2] = `hyper://${drive.key.hexSlice()}/${link[2]}`
    outgoingLinks.push(link[2])
  }

  let nLinks = 0
  for (const link of outgoingLinks) {
    let url = null
    try {
      url = new URL(link)
    } catch (err) {
      // TODO: I __really__ hope this error is generated by a relative path like `href="/about"`
      console.warn('skipping link', err)
    }
    if (!url) continue
    if (url.protocol !== 'hyper:') continue // Ignore non hyperspace linking.
    const key = `backlinks/${url.host}/${driveKey}_${file.replace(/\//g, PATH_SEP)}`
    const written = await writeEntry(key, `${drive.version}`)
    if (written) nLinks++
    index(url)
  }

  let nTokens = 0
  // Analyze and store n-gram index
  const sentences = sbd(text, { sanitize: true })
  for (const sentence of sentences) {
    for (const token of words(sentence.replace(MDLINK_EXP, '$1'))) {
      const isHashtag = !!token.match(/^#/)
      const indexName = isHashtag ? 'tags' : 'terms'
      const tokenPath = (isHashtag ? token.substr(1) : token).split('').join('/')
      const key = `${indexName}/${tokenPath}/${driveKey}_${file.replace(/\//g, PATH_SEP)}`
      const data = {
        d: stat.mtime.getTime(),
        v: drive.version,
        // TODO: I originally thought that storing a file offset of where the match occured
        // would improve lookup times for previews. Now I'm nolonger sure if there's a point
        // to storing anything in this entry besides the seq/version.
        m: seq
      }
      const written = await writeEntry(key, JSON.stringify(data))
      if (written) nTokens++
    }
  }
  if (nLinks || nTokens) log('Text obj analyzed: ', nLinks, nTokens, file)

  if (body.length > 1024) { // Limit previews to 1KB
    await setPreview(body.substr(0, 1021) + '...')
  } else {
    // Make a full copy of the text.. It's ok for blahbity sized texts I guess.
    await setPreview(body)
  }
}

// Very naive word tokenizer
function * words (text, min = 3, max = 16) {
  for (const match of text.matchAll(/\b(#?[\w-]+)\b/g)) {
    const word = match[1]
    if (word.length < min || word.length > max) continue
    yield word.toLowerCase()
  }
}

function * ngram (text, min = 3, max = 8) {
  let size = min
  do {
    for (let i = 0; i < text.length - size; i++) {
      const n = text.substr(i, size).toLowerCase()
      if (!n.match(/^[\d\w]+$/)) continue
      yield n
    }
  } while (size++ < max)
}

function * edgeNgram (text, min = 3, max = 8) {
  for (let size = min; size <= max; size++) {
    const exp = new RegExp(`\\b([\\w\\d]{${size}})`, 'g')
    for (const m of text.matchAll(exp)) {
      yield m[1].toLowerCase()
    }
  }
}

// Another interesting piece of software: https://github.com/jharrilim/sentiment
// https://rawgit.com/qminer/qminer/master/nodedoc/index.html
